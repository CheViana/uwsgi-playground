# Python uWSGI experiments: Part 0. Monitoring it all.

I decided to start on series of blogposts that shed light on some aspects of Python web server performance which I wish I had ability to read 4 years ago.
I will start with basics and gradually grow complexity.


Having a web application, one wonders how to monitor it's performance when it's running in production, 
and how to best configure web server to consume less resources while not dropping client connections.

Typical questions:
- how long do requests execute on avg?
- how much does page render time take?
- what is response time of this or that backend that's being called in web app? 
- which backend produces errors right now?
- why web app is crashing right now?
- what can be done so that same size cloud pod serves more requests reliably?


Let's try to answer first question on simplistic uWSGI server.
There's primitive Hello World uWSGI app in uwsgi-hello-world.py:

    import time


    def application(env, start_response):
        start_response('200 OK', [('Content-Type','text/html')])
        time.sleep(0.25)
        return [b"Hello World"]

And uWSGI configs in uwsgi-hello-world-configs.ini (also from uWSGI docs).

This code doesn't do anything useful but need to start from somewhere. We will add usefulness and complexity bit by bit.
Can use wsgi.py of your Django or Flask server or any other web app (wsgi = path/to/Django-app/wsgi.py), should be perfectly alright, and probably more interesting.

Provided uWSGI is installed e.g. via pip, and working directory is set to be directory with mentioned above files, can launch uWSGI server:

    uwsgi --ini uwsgi-hello-world-configs.ini

This should output similar to:

    ...
    uwsgi socket 0 bound to TCP address 127.0.0.1:9090
    *** Operational MODE: preforking+threaded ***
    WSGI app 0 (mountpoint='') ready in 0 seconds ...
    *** uWSGI is running in multiple interpreter mode ***
    spawned uWSGI master process (pid: ...)
    spawned uWSGI worker 1 (pid: ..., cores: 2)
    spawned uWSGI worker 2 (pid: ..., cores: 2)
    spawned uWSGI worker 3 (pid: ..., cores: 2)
    spawned uWSGI worker 4 (pid: ..., cores: 2)
    *** Stats server enabled on 127.0.0.1:9191 fd: ... ***

It means web server has launched and it listening to 127.0.0.1:9090.

Can visit hello-world server in local web browser - put http://127.0.0.1:9090/ in your browser URL - will see something like [pic 1].
Can also visit stats endpoint in local web browser - put http://127.0.0.1:9191/  in your browser URL - will see JSON with something like:

    ...
    "workers": [
        {
			"id":2,
			"pid":...,
			"accepting":1,
			"requests":1,
			"delta_requests":1,
            "avg_rt":71928
            ....
    ]
    ...

uWSGI outputs to console that it processed requests after we visited those pages:

    [pid: ...|app: 0|req: 1/1] 127.0.0.1 () {42 vars in 783 bytes} [Sun Jul 19 20:21:38 2020] GET / => generated 11 bytes in 250 msecs (HTTP/1.1 200) 1 headers in 44 bytes (2 switches on core 0)
    [pid: ...|app: 0|req: 1/2] 127.0.0.1 () {38 vars in 670 bytes} [Sun Jul 19 20:21:38 2020] GET /favicon.ico => generated 11 bytes in 143 msecs (HTTP/1.1 200) 1 headers in 44 bytes (2 switches on core 0)

uWSGI can also be configured to log those lines to a file (add log-file=... ? to .ini).


## Tools for real-time monitoring of web server: graphana, telegraf, influxdb (TICK stack)

There's plenty of monitoring solutions, some are all-in-one like DataDog or ... (forgot what we used). They provide tasty features and are generally production-ready without much hassle.
However for small-scale local experiments I want something I have full control over, and having some previous experience with TICK stack I picked it. (I was curious to try out Prometeus too but desided to focus on other things first. I'm sure I'll give it a go in future.)
TICK stack provides input points, storage and visualization tools for metrics.


For TICK stack:

    brew install influxdb  <-- Database for metrics
    brew install telegraf  <-- agent-collector of metrics
    brew install graphana  <-- UI for metrics exploration and plotting


Launch influxDB:

    influxd -config /usr/local/etc/influxdb.conf


Create database for local playground metrics (using CLI in other shell tab):

    influx -precision rfc3339
    Connected to http://localhost:8086 version v1.8.1
    InfluxDB shell version: v1.8.1
    > CREATE DATABASE localmetrics
    > SHOW DATABASES
    name: databases
    name
    ----
    _internal
    localmetrics
    >

It is now possible to run "INSERT ..." command using CLI, which will add metric reading to the database, some dummy metric. ...

We want to have uWSGI stats (data in that JSON) sent to database automatically, e.g. every 5 seconds, and in format that InfluxDB will understand.
Also uWSGI passively exposes stats data on socket 9191, so request on that socket will result in response with stats JSON, but if nobody is executing requests on 9191 socket - stats are not known to anybody (except uWSGI process itself because it uses stats to e.g. know when to respawn worker processes).
Need something that will query uWSGI stats endpoint and send it to InfluxDB database.
This is where Telegraf comes to light. Telegraf has ability to retrieve metrics data, transform it to format InfluxDB understands and send it over to InfluxDB database.
Telegraf has a bunch of input plugins: to watch over system cpu, to read log file tail, to listen to messages on socket, etc, and various web servers integrations, including uWSGI.
It's a matter of adding few lines to telegraf config and it will consume uWSGI stats.

https://github.com/influxdata/telegraf/blob/release-1.14/plugins/inputs/uwsgi/README.md

For now, we will start telegraf with with uWSGI input plugin only, and will add ability to consume other metrics in next chapters.
Let's use telegraf sample-config utility to compose it's configs:

    > telegraf -sample-config --input-filter uwsgi --output-filter influxdb > telegraf.conf
    > cat telegraf.conf
    ...
    # Read uWSGI metrics.
    [[inputs.uwsgi]]
    ## List with urls of uWSGI Stats servers. URL must match pattern:
    ## scheme://address[:port]
    ##
    ## For example:
    ## servers = ["tcp://localhost:5050", "http://localhost:1717", "unix:///tmp/statsock"]
    servers = ["tcp://127.0.0.1:1717"]

    ## General connection timout
    # timeout = "5s"

tcp://127.0.0.1:1717 part doesn't match where our uWSGI exposes stats on. It's http://127.0.0.1:9191. Need to update that in telegraf.conf file.
Another thing that needs to be updated is the address of where to write stats to:

    # Configuration for sending metrics to InfluxDB
    [[outputs.influxdb]]
    ## The full HTTP or UDP URL for your InfluxDB instance.
    ##
    ## Multiple URLs can be specified for a single cluster, only ONE of the
    ## urls will be written to each interval.
    # urls = ["unix:///var/run/influxdb.sock"]
    # urls = ["udp://127.0.0.1:8089"]
    urls = ["http://127.0.0.1:8086"]

    ## The target database for metrics; will be created as needed.
    ## For UDP url endpoint database needs to be configured on server side.
    database = "uWSGI"

I uncommented `urls = ["http://127.0.0.1:8086"]` line and added `database = "uWSGI"` so metrics from uWSGI stats server will flow in separate database.
Resulting config for telegrad is included in repo as telegraf.conf.

To keep uWSGI busy and get some interesting pictures, let's load uWSGI hello-world server using Apache AB requests generator:

> ab -c 5 -n 10000000 http://127.0.0.1:9090/


Looking in InfluxDB console output, can see:

    2020-07-20T01:18:26.727302Z	info	Executing query	{"log_id": "0O6K1AQG000", "service": "query", "query": "CREATE DATABASE uWSGI"}
    [httpd] 127.0.0.1 - - [19/Jul/2020:21:18:26 -0400] "POST /query HTTP/1.1" 200 57 "-" "Telegraf/1.14.5" e9bd1368-ca26-11ea-8005-88e9fe853b3a 428
    [httpd] 127.0.0.1 - - [19/Jul/2020:21:18:40 -0400] "POST /write?db=uWSGI HTTP/1.1" 204 0 "-" "Telegraf/1.14.5" f1a77b04-ca26-11ea-8006-88e9fe853b3a 137748
    [httpd] 127.0.0.1 - - [19/Jul/2020:21:18:50 -0400] "POST /write?db=uWSGI HTTP/1.1" 204 0 "-" "Telegraf/1.14.5" f79d5380-ca26-11ea-8007-88e9fe853b3a 7695
    [httpd] 127.0.0.1 - - [19/Jul/2020:21:19:00 -0400] "POST /write?db=uWSGI HTTP/1.1" 204 0 "-" "Telegraf/1.14.5" fd928134-ca26-11ea-8008-88e9fe853b3a 8534
    [httpd] 127.0.0.1 - - [19/Jul/2020:21:19:10 -0400] "POST /write?db=uWSGI HTTP/1.1" 204 0 "-" "Telegraf/1.14.5" 0388c512-ca27-11ea-8009-88e9fe853b3a 9345

So telegraf is busy writing uWSGI metrics to database but we can't see them yet. Let's improve on it.
Launch grafana web server:

    > cd path/to/dir/with/installed/graphana
    > bin/grafana-server

Navigating to http://localhost:3000/ in browser opens window with Grafana UI. Login using "admin" "admin" and create new pass as it asks to.
Pick "Configuration -> Data Sources -> Add data source" [pic 2]. Select InfluxDB.
Put "http://127.0.0.1:8086" in HTTP/URL input and database name "uWSGI" in "InfluxDB Details/Database" input [pic 3].
Click on "Save and Test" button in the bottom of the screen, green noty reading "Data source is working" should appear [pic 4].
Now Graphana can read metrics from database, let's visualize some of those.
Go to "Dashboards -> Manage dashboards", click on "New dashboard", click "New panel" [pic 5].
In that panel, select Query source - InfluxDB.
Modify query builder inputs [as on pic 6]: 
 - From default *uwsgi_workers*
 - Select field(*avg_rt*) mean()
It is also important to tell panel that metric it displays is measured in microseconds, in the right column expand "Axes", "Left Y", "Unit" - "microseconds" [pic 7].
Also nice to configure points thickness so that you can see them clearly ("Display" - "Line width" in the right column).
Give some meaningful name to that panel, save panel (click "Apply" in top right corner).
Now can view uWSGI dashboard at http://localhost:3000/dashboard/new?orgId=1&from=now-15m&to=now&refresh=10s
It in real time shows avg request time of all uwsgi workers (make sure refresh frequency selector is set to like 10 sec, tiny dropdown in top right corner of dashabord page).

uWSGI allows to monitor following metrics:
...